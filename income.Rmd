---
title: "Project Report-1"
author: "Aravindh Boya"
date: "6/26/2020"
output: word_document
---


# 1. INTRODUCTION












###### **Nature of data:**






###### **Attributes Info:**

















```{r setup, include=FALSE}
df = read.csv('C:/Users/aravi/OneDrive/Documents/special_topics/Datasets/income-evaluation.xls')

cat("The number of Null values in dataset is", sum(is.na(df)))

summary(df)
```

#### Data Preprocessing
```{r, include=FALSE}
df$income = as.factor(df$income)
df$income = as.numeric(df$income)

df$income = df$income - 1

table(df$income)
```

# 2. METHODS











```{r, echo=TRUE}
###### set seed to ensure you always have same random numbers generated
set.seed(22)  

train_ind = sample(seq_len(nrow(df)),size = 0.7 * nrow(df))  
###### creates the training dataset with row numbers stored in train_ind
train =df[train_ind,] 
test=df[-train_ind,]

table(train$income)
```

+  **Split the data:** To address our problem, Above I have divided the data set into two  samples, Training(70%) and Testing(30%). In our case we use for example 200 observations in the train data, 86 in the test data.


``` {r chi-squared, echo=TRUE}

```

+ ** Analysis: **



+ **Interpretation: ** 



## Classification Techniques

#### Logistic Regression









```{r glm, results="hide",message=F, warning=F}
logit_model = glm(
  as.factor(income) ~ workclass + education + marital.status  + occupation + relationship + race,
  data = train,
  family = "binomial"
)



logit_pred = predict(logit_model,newdata = test, type = "response")

logit_pred = ifelse(logit_pred > 0.5, 1, 0)



```

#### 	Cross Validation





```{r CV, results="hide",message=F, warning=F}
train_control_cv = trainControl(method = "cv", number = 3)

logit_model_cv = train(
  as.factor(income) ~  workclass + education + marital.status  + occupation + relationship + race,
  data = train,
  trControl = train_control_cv,
  method = "glm",
  family = binomial
)

logit_pred_cv = predict(logit_model_cv, newdata = test)

logit_pred_cv = ifelse(logit_pred_cv > 0.5, 1, 0)
```

#### Linear Descriminant Analysis







```{r lda, results="hide",message=F, warning=F}
library(MASS)

lda_model <- lda(income ~  workclass + education + marital.status  + occupation + relationship + race, data = train)

lda_predictions = predict(lda_model, newdata = test)

lda_predictions = lda_predictions$class
```

#### K-Nearest Neighbors









```{r KNN, results="hide",message=F, warning=F}
library(class)

dummy_data <- dummyVars(" ~ .", data = train)

train_oh <- data.frame(predict(dummy_data, newdata = train))

test_oh <- data.frame(predict(dummy_data, newdata = test))

cols_exc <- names(train_oh) %in% c("income")

train_knn <- train_oh[!cols_exc,]

test_knn <- test_oh[!cols_exc,]
train_labels <- train_oh$income
test_labels <- test_oh$income

knn_preds = knn(train_knn,test_knn,cl=as.factor(train_labels),k=3)
```



+ **Validation accuracy:**



+ **Test accuracy:**




# RESULTS

#### Plots for the Data set
```{r echo=TRUE,figures-side, fig.show="hold", out.width="50%"}
barplot(table(df$age), xlab = 'Age', ylab = 'Frequency', main = 'Age', col= "deepskyblue1")

barplot(table(as.factor(df$sex)), xlab = 'Sex', ylab = 'Frequency', main = 'Sex', col = c("#F67373", "#73D7F6"))

barplot(table(as.factor(df$income)), xlab = 'Income level', ylab = 'Frequency', main = 'Target Distribution', col = c("#F67373", "#73D7F6"))

barplot(table(as.factor(df$marital.status)), xlab = 'Marital status', ylab = 'Frequency', main = 'Marital Status Distribution', col =  brewer.pal(n = 7, name = "Reds"))

barplot(table(as.factor(df$race)), xlab = 'Race', ylab = 'Frequency', main = 'Race Distribution', col =  brewer.pal(n = 5, name = "PuBu"))

barplot(table(as.factor(df$education)), xlab = 'Education', ylab = 'Frequency', main = 'Education Distribution', col =  brewer.pal(n = 7, name = "Set3"))

hist(df$capital.gain, xlab = 'Capital gain', ylab = 'Frequency', main = 'Capital Gain histogram', col = 'deepskyblue')

hist(df$capital.loss, xlab = 'Capital loss', ylab = 'Frequency', main = 'Capital Loss histogram', col = 'deepskyblue3')

barplot(table(as.factor(df$relationship)), xlab = 'relationship', ylab = 'Frequency', main = 'relationship Distribution', col =  brewer.pal(n = 7, name = "Dark2"))

hist(df$hours.per.week, xlab = 'Hours per week', ylab = 'Frequency', main = 'Hours per week histogram', col = 'tomato2')

country_names = c('USA','Non-USA')
country_freq = c(dim(df %>%filter(as.integer(native.country) %in% c(40)))[1], dim(df %>%filter(!as.integer(native.country) %in% c(40)))[1])

barplot(country_freq, main = "Country Frequency", xlab = "Country", ylab = 'Frequency', names.arg = country_names, col = c("#F63C6E", "#D0F989"))

```


## Method Analysis





**Confusion Matrix:**





### Logistic Regression Analysis
```{r echo=TRUE}
table(test$income,logit_pred)
logit_reg_cm = confusionMatrix(as.factor(logit_pred), as.factor(test$income))
logit_reg_cm
logit_accuracy <- logit_reg_cm$overall[1]

cat("The logistic Regression accuracy is", logit_accuracy)
```

### Linear Discriminant Analysis
```{r echo=TRUE}
lda_cm = confusionMatrix(as.factor(lda_predictions), as.factor(test$income))
lda_cm
lda_accuracy <- lda_cm$overall[1]

cat("The Linear Discriminant Analysis accuracy is", lda_accuracy)
```

### KNN Analysis
```{r echo=TRUE}
knn_cm = confusionMatrix(as.factor(knn_preds), as.factor(test_labels))
knn_cm
knn_accuracy <- knn_cm$overall[1]

cat("The Knn accuracy is", knn_accuracy)
```




# 4. DISCUSSION 





#### Decision Tree





```{r DT, results="hide",message=F, warning=F}
library(rpart)

dt <- rpart(income ~  workclass + education + marital.status  + occupation + relationship + race, data = train)

dt_preds = predict(dt, test)

dt_preds = round(dt_preds)
```


### Decision Tree Analysis
```{r echo=TRUE}
dt_cm = confusionMatrix(as.factor(dt_preds), as.factor(test_labels))
dt_cm
dt_accuracy <- dt_cm$overall[1]

cat("The decision tree accuracy is", dt_accuracy)
```



# 5. REFERENCES CITED




